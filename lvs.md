# LVS手册 

    LVS手册是为了给LVS用户和开发者提供一个完整的参考手册。也希望大家提供LVS相关的好文章以及使用经验。
    
    如何给出合理的框架和有效的设计方法，来建立高性能、高可伸缩、高可用的网络服务，这是摆在研究者和系统设计者面前极富挑战性的任务。下面文章就是围绕这一任务展开的。


##可伸缩网络服务的设计与实现
    
    人类社会正在进入以网络为中心的信息时代，人们需要更快捷、更可靠、功能更丰富的网络服务。万维网的流行促进互联网使用的指数级增长，现在很多站点收到前所未有的访问负载，
    经常担心系统如何被扩展来满足不断增长的性能需求，同时系统如何保持24x7的可用性。未来的应用将需要更高的吞吐率、更好的交互性、更高的安全性，这要求服务平台具有更强的
    处理能力和更高的可用性。所以，如何给出合理的框架和有效的设计方法，来建立高性能、高可伸缩、高可用的网络服务，这是摆在研究者和系统设计者面前极富挑战性的任务。本文
    研究和设计的可伸缩网络服务便是围绕这一任务展开的。

1. [可伸缩网络服务的体系结构](#1)
    1. [可伸缩网络服务的定义](#1.1)
    2. [网络服务的需求](#1.2)
    3. [LVS集群的体系结构](#1.3)
        1. [LVS集群的通用结构](#1.3.1)
        2. [可伸缩Web和媒体服务](#1.3.2)
        3. [可伸缩Cache服务](#1.3.3)
        4. [可伸缩邮件服务](#1.3.4)
    4. [地理分布LVS集群的体系结构](#1.4)
        1. [体系结构](#1.4.1)
        2. [基于BGP的地理分布服务器集群调度](#1.4.2)
        3. [服务器集群间的负载均衡](#1.4.3)
    5. [小结](#1.5)
    
2. [IP负载均衡技术](#2)
    1. [通过NAT实现虚拟服务器（VS/NAT）](#2.1)
    2. [通过IP隧道实现虚拟服务器（VS/TUN）](#2.2)
    3. [通过直接路由实现虚拟服务器（VS/DR）](#2.3)
    4. [三种方法的优缺点比较](#2.4)
    5. [小结](#2.5)
    
3. [负载调度](#3)
    1. [内核中的连接调度算法](#3.1)
        1. [轮叫调度（Round-Robin Scheduling）](#3.1.1)
        2. [加权轮叫调度（Weighted Round-Robin Scheduling）](#3.1.2)
        3. [最小连接调度（Least-Connection Scheduling）](#3.1.3)
        4. [加权最小连接调度（Weighted Least-Connection Scheduling）](#3.1.4)
        5. [基于局部性的最少链接（Locality-Based Least Connections Scheduling）](#3.1.5)
        6. [带复制的基于局部性最少链接（Locality-Based Least Connections with Replication Scheduling）](#3.1.6)
        7. [目标地址散列调度（Destination Hashing Scheduling）](#3.1.7)
        8. [源地址散列调度（Source Hashing Scheduling）](#3.1.8)
    2. [动态反馈负载均衡算法](#3.2)
        1. [连接调度](#3.2.1)
        2. [动态反馈负载均衡机制](#3.2.2)
        3. [综合负载](#3.2.3)
        4. [权值计算](#3.2.4)
        5. [一个实现例子](#3.2.5)
    3. [小结](#3.3)
    
4. [IP虚拟服务器的实现和性能测试](#4)
    1. 系统实现的基本框架
    2. 系统实现的若干问题
    3. 性能测试
    4. LVS集群的应用
    5. 小结
    
5. [内核中的基于内容请求分发](#5)
    1. 基于内容的请求分发
    2. 内核中的基于内容请求分发KTCPVS
    3. KTCPVS的调度算法
    
6. [TCPHA的设计与实现](#6)



## <a name="1">1. 可伸缩网络服务的体系结构</a> 

    针对网络服务的可伸缩性、高可用性、可维护性和价格有效性需求，本章给出了可伸缩网络服务的体系结构和设计方法，它提供了负载平衡、可伸缩性和高可用性。



### <a name="1.1">1.1 可伸缩网络服务的定义</a> 

    可伸缩性（Scalability）是在当今计算机技术中经常用到的词汇。对于不同的人，可伸缩性有不同的含义。 现在，我们来定义可伸缩网络服务的含义。

    可伸缩网络服务是指网络服务能随着用户数目的增长而扩展其性能，如在系统中增加服务器、内存或硬盘等；整个系统很容易被扩展，无需重新设置整个系统，无需中断服务。换句话
    说，系统管理员扩展系统的操作对最终用户是透明的，他们不会知道系统的改变。

    可伸缩系统通常是高可用的系统。在部分硬件（如硬盘、服务器、子网络）和部分软件（如操作系统、服务进程）的失效情况下，系统可以继续提供服务，最终用户不会感知到整个服务
    的中断，除了正在失效点上处理请求的部分用户可能会收到服务处理失败，需要重新提交请求。Caching和复制是建立高可用系统的常用技术，建立多个副本会导致如何将原件的修改传
    播到多个副本上的问题。

    实现可伸缩网络服务的方法一般是通过一对多的映射机制，将服务请求流分而治之（Divide and Conquer）到多个结点上处理。一对多的映射可以在很多层次上存在，如主机名上的
    DNS系统、网络层的TCP/IP、文件系统等。虚拟（Virtual）是描述一对多映射机制的词汇，将多个实体组成一个逻辑上的、虚拟的整体。例如，虚存（Virtual Memory）是现代操作
    系统中最典型的一对多映射机制，虚存建立一个虚拟内存空间，将它映射到多个物理内存上。


### <a name="1.2">1.2 网络服务的需求</a> 

    随着Internet的飞速发展和对我们生活的深入影响，越来越多的个人在互联网上购物、娱乐、休闲、与人沟通、获取信息；越来越多的企业把他们与顾客和业务伙伴之间的联络搬到互
    联网上，通过网络来完成交易，建立与客户之间的联系。互联网的用户数和网络流量正以几何级数增长，这对网络服务的可伸缩性提出很高的要求。例如，比较热门的Web站点会因为
    被访问次数急剧增长而不能及时处理用户的请求，导致用户进行长时间的等待，大大降低了服务质量。另外，随着电子商务等关键性应用在网上运行，任何例外的服务中断都将造成不可
    估量的损失，服务的高可用性也越来越重要。所以，对用硬件和软件方法实现高可伸缩、高可用网络服务的需求不断增长，这种需求可以归结以下几点：

        * 可伸缩性（Scalability），当服务的负载增长时，系统能被扩展来满足需求，且不降低服务质量。
        * 高可用性（Availability），尽管部分硬件和软件会发生故障，整个系统的服务必须是每天24小时每星期7天可用的。
        * 可管理性（Manageability），整个系统可能在物理上很大，但应该容易管理。
        * 价格有效性（Cost-effectiveness），整个系统实现是经济的、易支付的。

    单服务器显然不能处理不断增长的负载。这种服务器升级方法有下列不足：
            一是升级过程繁琐，机器切换会使服务暂时中断，并造成原有计算资源的浪费；
            二是越往高端的服务器，所花费的代价越大；
            三是一旦该服务器或应用软件失效，会导致整个服务的中断。

    通过高性能网络或局域网互联的服务器集群正成为实现高可伸缩的、高可用网络服务的有效结构。这种松耦合结构比紧耦合的多处理器系统具有更好的伸缩性和性能价格比，组成集群的
    PC服务器或RISC服务器和标准网络设备因为大规模生产，价格低，具有很高的性能价格比。但是，这里有很多挑战性的工作，如何在集群系统实现并行网络服务，它对外是透明的，它
    具有良好的可伸缩性和可用性。

    针对上述需求，我们给出了基于IP层和基于内容请求分发的负载平衡调度解决方法，并在Linux内核中实现了这些方法，将一组服务器构成一个实现可伸缩的、高可用网络服务的服务器
    集群，我们称之为Linux虚拟服务器（Linux Virtual Server）。在LVS集群中，使得服务器集群的结构对客户是透明的，客户访问集群提供的网络服务就像访问一台高性能、高可用
    的服务器一样。客户程序不受服务器集群的影响不需作任何修改。系统的伸缩性通过在服务机群中透明地加入和删除一个节点来达到，通过检测节点或服务进程故障和正确地重置系统
    达到高可用性。

### <a name="1.3">1.3 LVS集群的体系结构</a> 

    下面先给出LVS集群的通用结构，讨论了它的设计原则和相应的特点；然后将LVS集群应用于建立可伸缩的Web、Media、Cache和Mail等服务。

#### <a name="1.3.1">1.3.1 LVS集群的通用结构</a> 
    
    LVS集群采用IP负载均衡技术和基于内容请求分发技术。调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器
    自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器。


![](/image/lvs/2.1.jpg)
        图2.1：LVS集群的体系结构


    为此，在设计时需要考虑系统的透明性、可伸缩性、高可用性和易管理性。LVS集群的体系结构如图2.1所示，它有三个主要组成部分：

        *负载调度器（load balancer），它是整个集群对外面的前端机，负责将客户的请求发送到一组服务器上执行，而客户认为服务是来自一个IP地址上的。它可以是用IP负载均衡
                技术的负载调度器，也可以是基于内容请求分发的负载调度器，还可以是两者的结合。
        *服务器池（server pool），是一组真正执行客户请求的服务器，执行的服务有WEB、MAIL、FTP和DNS等。
        *后端存储（backend storage），它为服务器池提供一个共享的存储区，这样很容易使得服务器池拥有相同的内容，提供相同的服务。

    调度器采用IP负载均衡技术、基于内容请求分发技术或者两者相结合。在IP负载均衡技术中，需要服务器池拥有相同的内容提供相同的服务。当客户请求到达时，调度器只根据负载情
    况从服务器池中选出一个服务器，将该请求转发到选出的服务器，并记录这个调度；当这个请求的其他报文到达，也会被转发到前面选出的服务器。在基于内容请求分发技术中，服务
    器可以提供不同的服务，当客户请求到达时，调度器可根据请求的内容和服务器的情况选择服务器执行请求。因为所有的操作都是在操作系统核心空间中将完成的，它的调度开销很小，
    所以它具有很高的吞吐率。

    服务器池的结点数目是可变的。当整个系统收到的负载超过目前所有结点的处理能力时，可以在服务器池中增加服务器来满足不断增长的请求负载。对大多数网络服务来说，结点与结
    点间不存在很强的相关性，所以整个系统的性能可以随着服务器池的结点数目增加而线性增长。

    后端存储通常用容错的分布式文件系统，如AFS、GFS、Coda和Intermezzo等。分布式文件系统为各服务器提供共享的存储区，它们访问分布式文件系统就像访问本地文件系统一
    样。同时，分布式文件系统提供良好的伸缩性和可用性。然而，当不同服务器上的应用程序同时访问分布式文件系统上同一资源时，应用程序的访问冲突需要消解才能使得资源处于一
    致状态。这需要一个分布式锁管理器（Distributed Lock Manager），它可能是分布式文件系统内部提供的，也可能是外部的。开发者在写应用程序时，可以使用分布式锁管理器来
    保证应用程序在不同结点上并发访问的一致性。

    负载调度器、服务器池和分布式文件系统通过高速网络相连，如100Mbps交换机、Myrinet、CompactNET和Gigabit交换机等。使用高速的网络，主要为避免当系统规模扩大时互联
    网络成为瓶颈。

    Graphic Monitor是为系统管理员提供整个集群系统的监视器，它可以监视系统中每个结点的状况。Graphic Monitor是基于浏览器的，所以无论管理员在本地还是异地都可以监测
    系统的状况。为了安全的原因，浏览器要通过HTTPS（Secure HTTP）协议和身份认证后，才能进行系统监测，并进行系统的配置和管理。

        *为什么使用层次的体系结构
                层次的体系结构可以使得层与层之间相互独立，允许在一个层次的已有软件在不同的系统中被重用。例如，调度器层提供了负载平衡、可伸缩性和高可用性等，在服务器
                层可以运行不同的网络服务，如Web、Cache、Mail和Media等，来提供不同的可伸缩网络服务。

        *为什么是共享存储
                共享存储如分布式文件系统在这个LVS集群系统是可选项。当网络服务需要有相同的内容，共享存储是很好的选择，否则每台服务器需要将相同的内容复制到本地硬盘上。
                当系统存储的内容越多，这种不共享结构（Shared-nothing Structure）的代价越大，因为每台服务器需要一样大的存储空间，所有的更新需要涉及到每台服务器，
                系统的维护代价也很高。

                共享存储为服务器组提供统一的存储空间，这使得系统的维护工作比较轻松，如Webmaster只需要更新共享存储中的页面，对所有的服务器都有效。分布式文件系统提
                供良好的伸缩性和可用性，当分布式文件系统的存储空间增加时，所有服务器的存储空间也随之增大。对于大多数Internet服务来说，它们都是读密集型
                （Read-intensive）的应用，分布式文件系统在每台服务器使用本地硬盘作Cache（如2Gbytes的空间），可以使得访问分布式文件系统本地的速度接近于访问本地
                硬盘。

        *高可用性
                集群系统的特点是它在软硬件上都有冗余。系统的高可用性可以通过检测节点或服务进程故障和正确地重置系统来实现，使得系统收到的请求能被存活的结点处理。
                通常，我们在调度器上有资源监视进程来时刻监视各个服务器结点的健康状况，当服务器对ICMP ping不可达时或者她的网络服务在指定的时间没有响应时，资源
                监视进程通知操作系统内核将该服务器从调度列表中删除或者失效。这样，新的服务请求就不会被调度到坏的结点。资源监测程序能通过电子邮件或传呼机向管理
                员报告故障，一旦监测到服务进程恢复工作，通知调度器将其加入调度列表进行调度。另外，通过系统提供的管理程序，管理员可发命令随时将一台机器加入服务
                或切出服务，很方便进行系统维护。

                现在前端的调度器有可能成为系统的单一失效点。为了避免调度器失效导致整个系统不能工作，我们需要设立调度器的备份。两个心跳进程（Heartbeat Daemon）
                分别在主、从调度器上运行，它们通过串口线和UDP等心跳线来相互汇报各自的健康情况。当从调度器不能听得主调度器的心跳时，从调度器会接管主调度器的工作
                来提供负载调度服务。这里，一般通过ARP欺骗（Gratuitous ARP）来接管集群的Virtual IP Address。当主调度器恢复时，这里有两种方法，一是主调度器自动
                变成从调度器，二是从调度器释放Virtual IP Address，主调度器收回Virtual IP Address并提供负载调度服务。然而，当主调度器故障后或者接管后，会导致已
                有的调度信息丢失，这需要客户程序重新发送请求。


#### <a name="1.3.2">1.3.2 可伸缩Web和媒体服务</a> 

    基于LVS可伸缩Web和媒体服务的体系结构如图2.2所示：在前端是一个负载调度器，一般采用IP负载均衡技术来获得整个系统的高吞吐率；在第二层是服务器池，Web服务和媒体服
    务分别运行在每个结点上；第三层是数据存储，通过分布式文件系统使得每个服务器结点共享相同的数据。集群中结点间是通过高速网络相连的。

![](/image/lvs/2.2.jpg)
    图2.2：基于LVS的可伸缩Web和媒体集群

    分布式文件系统提供统一的存储空间，这使得系统的维护工作比较方便，且系统运行比较高效。当所有服务器结点超载时，管理员可以很快地加入新的结点来处理请求，而无需将Web
    文档等复制到结点的本地硬盘上。Webmaster可以看到统一的文档存储空间，维护和更新页面比较方便，对分布式文件系统中页面的修改对所有的服务器都有效。大的媒体文件（如视
    频文件）分段存储在分布式文件系统的多个结点上，可以提高文件系统的性能和文件服务器间的负载均衡。

    IP负载调度器（即VS/DR方法，将在下一章详细叙述）可以分别将Web服务和媒体服务负载均衡地分发到各个服务器上，服务器将响应数据直接返回给客户，这样可以极大地提高系统
    的吞吐率。

    Real公司以其高压缩比的音频视频格式和音频视频播放器RealPlayer而闻名。Real公司正在使用以上结构将由20台服务器组成的LVS可伸缩Web和媒体集群，为其全球用户提供Web
    和音频视频服务。Real公司的高级技术主管声称LVS击败所有他们尝试的商品化负载均衡产品。

#### <a name="1.3.3">1.3.3 可伸缩Cache服务</a> 
    
    有效的网络Cache系统可以大大地减少网络流量、降低响应延时以及服务器的负载。但是，若Cache服务器超载而不能及时地处理请求，反而会增加响应延时。所以，Cache服务的可
    伸缩性很重要，当系统负载不断增长时，整个系统能被扩展来提高Cache服务的处理能力。尤其，在主干上的Cache服务可能需要几个Gbps的吞吐率，单台服务器（如SUN目前最高端
    的Enterprise 10000服务器）远不能达到这个吞吐率。可见，通过PC服务器集群实现可伸缩Cache服务是很有效的方法，也是性能价格比最高的方法。

    基于LVS可伸缩Cache集群的体系结构如图2.3所示：在前端是一个负载调度器，一般采用IP负载均衡技术来获得整个系统的高吞吐率；在第二层是Cache服务器池，一般Cache服务器
    放置在接近主干Internet连接处，它们可以分布在不同的网络中。调度器可以有多个，放在离客户接近的地方，可实现透明的Cache服务。


![](/image/lvs/2.3.jpg)
    图2.3：基于LVS的可伸缩Cache集群

    Cache服务器采用本地硬盘来存储可缓存的对象，因为存储可缓存的对象是写操作，且占有一定的比例，通过本地硬盘可以提高I/O的访问速度。Cache服务器间有专用的多播通道，
    通过ICP协议（Internet Cache Protocol）来交互信息。当一台Cache服务器在本地硬盘中未命中当前请求时，它可以通过ICP查询其他Cache服务器是否有请求对象的副本，若存
    在，则从邻近的Cache服务器取该对象的副本，这样可以进一步提高Cache服务的命中率。

    为150多所大学和地区服务的英国国家Cache网在1999年11月用以上LVS结构实现可伸缩的Cache集群，只用了原有50多台相互独立Cache服务器的一半，用户反映网络速度跟夏天
    一样快（学生放暑假）。可见，通过负载调度可以摸平单台服务器访问的毛刺（Burst），提高整个系统的资源利用率。

#### <a name="1.3.4">1.3.4 可伸缩邮件服务</a> 

    随着Internet用户不断增长，很多ISP面临他们邮件服务器超载的问题。当邮件服务器不能容纳更多的用户帐号时，有些ISP买更高档的服务器来代替原有的，
    将原有服务器的信息（如用户邮件）迁移到新服务器是很繁琐的工作，会造成服务的中断；有些ISP设置新的服务器和新的邮件域名，新的邮件用户放置在新
    的服务器上，如上海电信现在用不同的邮件服务器public1.sta.net.cn、public2.sta.net.cn到public9.sta.net.cn放置用户的邮件帐号，这样静态
    地将用户分割到不同的服务器上，会造成邮件服务器负载不平衡，系统的资源利用率低，对用户来说邮件的地址比较难记。


![](/image/lvs/2.4.jpg)
    图2.4：基于LVS的可伸缩邮件集群

    可以利用LVS框架实现可伸缩邮件服务。它的体系结构如图2.4所示：在前端是一个采用IP负载均衡技术的负载调度器；在第二层是服务器池，有LDAP
    （Light-weight Directory Access Protocol）服务器和一组邮件服务器。第三层是数据存储，通过分布式文件系统来存储用户的邮件。集群中
    结点间是通过高速网络相连的。

    用户的信息如用户名、口令、主目录和邮件容量限额等存储在LDAP服务器中，可以通过HTTPS让管理员进行用户管理。在各个邮件服务器上运行SMTP
    （Simple Mail Transfer Protocol）、POP3（Post Office Protocol version 3）、IMAP4（Internet Message Access Protocol version 4）
    和HTTP服务。SMTP接受和转发用户的邮件，SMTP服务进程查询LDAP服务器获得用户信息，再存储邮件。POP3和IMAP4通过LDAP服务器获得用户信
    息，口令验证后，处理用户的邮件访问请求。SMTP、POP3和IMAP4服务进程需要有机制避免用户邮件的读写冲突。HTTP服务是让用户通过浏览器可以
    访问邮件。负载调度器将四种服务请求负载均衡地调度到各个服务器上。

    系统中可能的瓶颈是LDAP服务器，对LDAP服务中B+树的参数进行优化，再结合高端的服务器，可以获得较高的性能。若分布式文件系统没有多个存储结点
    间的负载均衡机制，则需要相应的邮件迁移机制来避免邮件访问的倾斜。

    这样，这个集群系统对用户来说就像一个高性能、高可靠的邮件服务器（上海电信只要用一个邮件域名public.sta.net.cn）。当邮件用户不断增长时，
    只要在集群中增加服务器结点和存储结点。


### <a name="1.4">1.4 地理分布LVS集群的体系结构</a> 

    由于互联网用户分布在世界各地，通过地理分布的服务器让用户访问就近的服务器，来节省网络流量和提高响应速度。以下，我们给出地理分布的LVS集群
    系统，通过BGP路由插入使得用户访问离他们最近的服务器集群，并提供服务器集群之间的负载平衡。

#### <a name="1.4.1">1.4.1</a> 体系结构

    地理分布LVS集群的体系结构如图2.5所示：有三个LVS集群系统分布在Internet上，他们一般放置在不同区域的Internet数据中心
    （Internet Data Center）中，例如他们分别放在中国、美国和德国的三个不同的IDC中。三个LVS集群系统都有自己的分布式文件系统，它们的内容是
    相互复制的，提供相同的网络服务。它们共享一个Virtual IP Address来提供网络服务。当用户通过Virtual IP Address访问网络服务，离用户最近
    的LVS集群提供服务。例如，中国的用户访问在中国的LVS集群系统，美国的用户使用美国的LVS集群系统，这一切对用户来说是透明的。

![](/image/lvs/2.5.jpg)
    图2.5：地理分布LVS集群的体系结构

    地理分布LVS集群系统可以带来以下好处：
        *使得用户访问离他们最近的系统，对用户来说体验到更快的响应速度，对服务提供商来说节约网络带宽，降低成本。
        *避免灾难导致系统中止服务。当一个地点发生地震、火灾等使得系统或者网络连接瘫痪时，所有的用户访问可以很快由其他地点的LVS集群来提供。
          除了已建立的连接中断以外，这一切对用户来说都是透明的。



#### <a name="1.4.2">1.4.2</a> 基于BGP的地理分布服务器集群调度

    BGP（Border Gateway Protocol）是用于自治系统（Autonomous Systems）之间交换路由信息的协议，BGP可以设置路由策略，如政策、安全和经济上
    的考虑。

    我们可以利用BGP协议在Internet的BGP路由器插入到Virtual IP Address的路由信息。在不同区域的LVS集群向它附近的BGP路由器广播到Virtual 
    IP Address的路由信息，这样就存在多条到Virtual IP Address的路径，Internet的BGP路由器会根据评价函数选出最近的一条路径。这样，我们
    可以使得用户访问离他们最近的LVS集群。当一个LVS集群系统失效时，它的路由信息自然不会在Internet的BGP路由器中交换，BGP路由器会选择
    其他到Virtual IP Address的路径。这样，可以做到抗灾害性（Disaster Tolerance）。

![](/image/lvs/2.6.jpg)
    图2.6：基于BGP的地理分布服务器集群调度例子
    下面我们举一个基于BGP的地理分布服务器集群调度例子，它也是我们在实验室中测试的。测试的例子如图2.6所示，R11、R22分别在不同自治系统AS1和AS2中的Internet服务提供商（Internet Service Provider），R31和R32表示在自治系统AS3中两个ISP。两个LVS集群系统分别放置在两个不同数据中心IDC1和IDC2中，LB1和LB2分别是两个LVS集群系统的负载调度器，它们对外提供网络服务的IP地址为10.101.4.1。在第一个集群中，请求被调度到服务器10.101.5.11和10.101.5.12执行。在第二个集群中，请求被调度到服务器10.101.6.11和10.101.6.12。10.101.4.1是在自治系统AS5的网络10.101.4.0/24中。LB1上的BGP服务进程将网络10.101.4.0/24的路由信息广播到邻近的AS1，LB2上的BGP服务进程将网络10.101.4.0/24的路由信息广播到邻近的AS2中。
    
    我们在R31端口10.101.3.1相连的BGP路由器上查到AS5的自治系统路径为3→1→5。在与R32端口10.101.3.9相连的BGP路由器上查到AS5的自治系统路径为3→2→5，并在该路由器上访问10.101.4.1上提供的Web服务，是由服务器10.101.6.11处理的。当我们关掉LB2后，在该BGP路由器上查到AS5的自治系统路径变为3→1→5。
#### <a name="1.4.3">1.4.3</a> 服务器集群间的负载均衡

    通过BGP插入路由信息的方法可以使得用户访问邻近的服务器集群，但是用户访问存在突发性，在某个区域的访问高峰可能会导致该区域的服务器集群系统超载，而其他服务器集群系统可能处于低负载状态，这时与其将请求在超载系统上排队等候，不如将请求送到远处的低负载系统上执行，可以提高响应速度。例如，中国用户在白天时间的一段访问高峰使得在中国的镜像服务器集群系统超载，而此时美国是晚上时间其镜像服务器集群系统处于低负载状态。

    我们提出通过IP隧道的方法将请求从一个调度器转发到另一个调度器，再调度到真实服务器上执行。在下一章中将详细描述如何通过IP隧道作IP负载均衡调度。在通过IP隧道转发请求前，各个服务器集群需要定时交换负载信息（如2分钟交换一次），当确信远处的集群系统处于低负载状态，再转发请求。例如，当本地集群系统的综合负载大于1.1和远处集群系统的负载小于0.7时，调度器通过IP隧道将新的请求转发到远处的集群系统。若远处集群系统的负载超过0.7时，停止转发请求。当本地集群系统的负载降至1.0时，也停止转发请求，由本地服务器处理。这样，基本上可以避免两个调度器间相互转发一个请求。即使两个调度器间相互转发一个请求报文的例外情况发生，报文的TTL会降到零，报文被丢掉。


### <a name="1.5">1.5 小结</a> 

    我们分析了现在和将来网络服务的需求，提出可伸缩网络服务的体系结构，分为负载调度器、服务器池和后端存储三层结构。负载调度器采用IP负载均衡技术和基于内容请求分发技术。它的实现将在Linux操作系统进行，将一组服务器组成一个高可伸缩的、高可用的服务器，故称之为Linux Virtual Server。它提供了负载平衡、可伸缩性和高可用性，可以应用于建立很多可伸缩网络服务，如Web、Cache、Mail和Media等服务。

    在此基础上，我们给出了地理分布的LVS集群系统，通过BGP插入路由信息的方法可以使得用户访问邻近的服务器集群，通过IP隧道实现服务器集群间的负载均衡，进一步提高响应速度。地理分布的LVS集群系统可以节约网络带宽，改善网络服务质量，有很好的抗灾害性。


## <a name="2">2. IP负载均衡技术</a> 

    上一章节讲述了可伸缩网络服务的几种结构，它们都需要一个前端调度器。在调度器的实现技术中，IP负载均衡技术是效率最高的。在已有的IP负载均衡技术中有通过网络地址转换（Network Address Translation）将一组服务器构成一个高性能的、高可用的虚拟服务器，我们称之为VS/NAT技术（Virtual Server via Network Address Translation），大多数商品化的IP负载均衡调度器产品都是使用此方法，如Cisco的LocalDirector、F5的Big/IP和Alteon的ACEDirector。在分析VS/NAT的缺点和网络服务的非对称性的基础上，我们提出通过IP隧道实现虚拟服务器的方法VS/TUN（Virtual Server via IP Tunneling），和通过直接路由实现虚拟服务器的方法VS/DR（Virtual Server via Direct Routing），它们可以极大地提高系统的伸缩性。

    本章节将描述三种IP负载均衡技术VS/NAT、VS/TUN和VS/DR的工作原理，以及它们的优缺点。在以下描述中，我们称客户的socket和服务器的socket之间的数据通讯为连接，无论它们是使用TCP还是UDP协议。


### <a name="2.1">2.1 通过NAT实现虚拟服务器（VS/NAT）</a> 

    由于IPv4中IP地址空间的日益紧张和安全方面的原因，很多网络使用保留IP地址（10.0.0.0/255.0.0.0、172.16.0.0/255.128.0.0和192.168.0.0/255.255.0.0）[64, 65, 66]。这些地址不在Internet上使用，而是专门为内部网络预留的。当内部网络中的主机要访问Internet或被Internet访问时，就需要采用网络地址转换（Network Address Translation, 以下简称NAT），将内部地址转化为Internets上可用的外部地址。NAT的工作原理是报文头（目标地址、源地址和端口等）被正确改写后，客户相信它们连接一个IP地址，而不同IP地址的服务器组也认为它们是与客户直接相连的。由此，可以用NAT方法将不同IP地址的并行网络服务变成在一个IP地址上的一个虚拟服务。

    VS/NAT的体系结构如图3.1所示。在一组服务器前有一个调度器，它们是通过Switch/HUB相连接的。这些服务器提供相同的网络服务、相同的内容，即不管请求被发送到哪一台服务器，执行结果是一样的。服务的内容可以复制到每台服务器的本地硬盘上，可以通过网络文件系统（如NFS）共享，也可以通过一个分布式文件系统来提供。

![](/image/lvs/3.1.jpg)
    图3.1：VS/NAT的体系结构

    客户通过Virtual IP Address（虚拟服务的IP地址）访问网络服务时，请求报文到达调度器，调度器根据连接调度算法从一组真实服务器中选出一台服务器，将报文的目标地址Virtual IP Address改写成选定服务器的地址，报文的目标端口改写成选定服务器的相应端口，最后将修改后的报文发送给选出的服务器。同时，调度器在连接Hash表中记录这个连接，当这个连接的下一个报文到达时，从连接Hash表中可以得到原选定服务器的地址和端口，进行同样的改写操作，并将报文传给原选定的服务器。当来自真实服务器的响应报文经过调度器时，调度器将报文的源地址和源端口改为Virtual IP Address和相应的端口，再把报文发给用户。我们在连接上引入一个状态机，不同的报文会使得连接处于不同的状态，不同的状态有不同的超时值。在TCP连接中，根据标准的TCP有限状态机进行状态迁移；在UDP中，我们只设置一个UDP状态。不同状态的超时值是可以设置的，在缺省情况下，SYN状态的超时为1分钟，ESTABLISHED状态的超时为15分钟，FIN状态的超时为1分钟；UDP状态的超时为5分钟。当连接终止或超时，调度器将这个连接从连接Hash表中删除。

    这样，客户所看到的只是在Virtual IP Address上提供的服务，而服务器集群的结构对用户是透明的。对改写后的报文，应用增量调整Checksum的算法调整TCP Checksum的值，避免了扫描整个报文来计算Checksum的开销。

    在一些网络服务中，它们将IP地址或者端口号在报文的数据中传送，若我们只对报文头的IP地址和端口号作转换，这样就会出现不一致性，服务会中断。所以，针对这些服务，需要编写相应的应用模块来转换报文数据中的IP地址或者端口号。我们所知道有这个问题的网络服务有FTP、IRC、H.323、CUSeeMe、Real Audio、Real Video、Vxtreme / Vosiac、VDOLive、VIVOActive、True Speech、RSTP、PPTP、StreamWorks、NTT AudioLink、NTT SoftwareVision、Yamaha MIDPlug、iChat Pager、Quake和Diablo。

    下面，举个例子来进一步说明VS/NAT，如图3.2所示：
![](/image/lvs/3.2.jpg)
    图3.2：VS/NAT的例子

    VS/NAT的配置如下表所示，所有到IP地址为202.103.106.5和端口为80的流量都被负载均衡地调度的真实服务器172.16.0.2:80和172.16.0.3:8000上。目标地址为202.103.106.5:21的报文被转移到172.16.0.3:21上。而到其他端口的报文将被拒绝。

|Protocol|Virtual IP Address|Port|Real IP Address|Port|Weight|
|:---:|:---:|:---:|:---:|:---:|:---:|
|TCP|202.103.106.5|80|172.16.0.2|80|1
| -| -|- |172.16.0.3|8000|2
|TCP|202.103.106.5|21|172.16.0.3||21|1 

    从以下的例子中，我们可以更详细地了解报文改写的流程。

    访问Web服务的报文可能有以下的源地址和目标地址：

|SOURCE|202.100.1.2:3456|DEST|202.103.106.5:80
|:---:|:---:|:---:|:---:|:---:|:---:|

    调度器从调度列表中选出一台服务器，例如是172.16.0.3:8000。该报文会被改写为如下地址，并将它发送给选出的服务器。

|SOURCE|202.100.1.2:3456|DEST|172.16.0.3:8000
|:---:|:---:|:---:|:---:|:---:|:---:|

    从服务器返回到调度器的响应报文如下：

|SOURCE|172.16.0.3:8000|DEST|202.100.1.2:3456
|:---:|:---:|:---:|:---:|:---:|:---:|

    响应报文的源地址会被改写为虚拟服务的地址，再将报文发送给客户：

|SOURCE|202.103.106.5:80|DEST|202.100.1.2:3456
|:---:|:---:|:---:|:---:|:---:|:---:|

    这样，客户认为是从202.103.106.5:80服务得到正确的响应，而不会知道该请求是服务器172.16.0.2还是服务器172.16.0.3处理的。


### <a name="2.2">2.2 通过IP隧道实现虚拟服务器（VS/TUN）</a> 

    在VS/NAT的集群系统中，请求和响应的数据报文都需要通过负载调度器，当真实服务器的数目在10台和20台之间时，负载调度器将成为整个集群系统的新瓶颈。大多数Internet服务都有这样的特点：请求报文较短而响应报文往往包含大量的数据。如果能将请求和响应分开处理，即在负载调度器中只负责调度请求而响应直接返回给客户，将极大地提高整个集群系统的吞吐量。

    IP隧道（IP tunneling）是将一个IP报文封装在另一个IP报文的技术，这可以使得目标为一个IP地址的数据报文能被封装和转发到另一个IP地址。IP隧道技术亦称为IP封装技术（IP encapsulation）。IP隧道主要用于移动主机和虚拟私有网络（Virtual Private Network），在其中隧道都是静态建立的，隧道一端有一个IP地址，另一端也有唯一的IP地址。

    我们利用IP隧道技术将请求报文封装转发给后端服务器，响应报文能从后端服务器直接返回给客户。但在这里，后端服务器有一组而非一个，所以我们不可能静态地建立一一对应的隧道，而是动态地选择一台服务器，将请求报文封装和转发给选出的服务器。这样，我们可以利用IP隧道的原理将一组服务器上的网络服务组成在一个IP地址上的虚拟网络服务。VS/TUN的体系结构如图3.3所示，各个服务器将VIP地址配置在自己的IP隧道设备上。

![](/image/lvs/3.3.jpg)
    图3.3：VS/TUN的体系结构

    VS/TUN的工作流程如图3.4所示：它的连接调度和管理与VS/NAT中的一样，只是它的报文转发方法不同。调度器根据各个服务器的负载情况，动态地选择一台服务器，将请求报文封装在另一个IP报文中，再将封装后的IP报文转发给选出的服务器；服务器收到报文后，先将报文解封获得原来目标地址为VIP的报文，服务器发现VIP地址被配置在本地的IP隧道设备上，所以就处理这个请求，然后根据路由表将响应报文直接返回给客户。

![](/image/lvs/3.4.jpg)
    图3.4：VS/TUN的工作流程

    在这里，请求报文的目标地址为VIP，响应报文的源地址也为VIP，所以响应报文不需要作任何修改，可以直接返回给客户，客户认为得到正常的服务，而不会知道是哪一台服务器处理的。

    在VS/TUN中，响应报文根据服务器的路由表直接返回给客户，而不经过负载调度器，所以负载调度器只处于从客户到服务器的半连接中，VS/TUN的TCP状态迁移与VS/NAT的不同。我们给出半连接的TCP有限状态机，如图3.5所示，圈表示状态，箭头表示状态间的转换，箭头上的标识表示在当前状态上收到该标识的输入，迁移到下一个状态。VS/TUN的TCP状态迁移是按照半连接的TCP有限状态机进行的。

![](/image/lvs/3.5.jpg)
    图3.5：半连接的TCP有限状态机


### <a name="2.3">2.3 通过直接路由实现虚拟服务器（VS/DR）</a> 

    跟VS/TUN方法相同，VS/DR利用大多数Internet服务的非对称特点，负载调度器中只负责调度请求，而服务器直接将响应返回给客户，可以极大地提高整个集群系统的吞吐量。该方法与IBM的NetDispatcher产品中使用的方法类似，但IBM的NetDispatcher是非常昂贵的商品化产品，我们也不知道它内部所使用的机制，其中有些是IBM的专利。

    VS/DR的体系结构如图3.6所示：调度器和服务器组都必须在物理上有一个网卡通过不分段的局域网相连，即通过交换机或者高速的HUB相连，中间没有隔有路由器。VIP地址为调度器和服务器组共享，调度器配置的VIP地址是对外可见的，用于接收虚拟服务的请求报文；所有的服务器把VIP地址配置在各自的Non-ARP网络设备上，它对外面是不可见的，只是用于处理目标地址为VIP的网络请求。

![](/image/lvs/3.6.jpg)
    图3.6：VS/DR的体系结构

    VS/DR的工作流程如图3.7所示：它的连接调度和管理与VS/NAT和VS/TUN中的一样，它的报文转发方法又有不同，将报文直接路由给目标服务器。在VS/DR中，调度器根据各个服务器的负载情况，动态地选择一台服务器，不修改也不封装IP报文，而是将数据帧的MAC地址改为选出服务器的MAC地址，再将修改后的数据帧在与服务器组的局域网上发送。因为数据帧的MAC地址是选出的服务器，所以服务器肯定可以收到这个数据帧，从中可以获得该IP报文。当服务器发现报文的目标地址VIP是在本地的网络设备上，服务器处理这个报文，然后根据路由表将响应报文直接返回给客户。

![](/image/lvs/3.7.jpg)
    图3.7：VS/DR的工作流程

    在VS/DR中，请求报文的目标地址为VIP，响应报文的源地址也为VIP，所以响应报文不需要作任何修改，可以直接返回给客户，客户认为得到正常的服务，而不会知道是哪一台服务器处理的。

    VS/DR负载调度器也只处于从客户到服务器的半连接中，按照半连接的TCP有限状态机进行状态迁移。

### <a name="2.4">2.4 三种方法的优缺点比较</a> 

    三种IP负载均衡技术的优缺点归纳在下表中：

|-|VS/NAT|VS/TUN|VS/DR|
|:---:|:---:|:---:|
|Server|any|Tunneling|Non-arp device
|server network|private|LAN/WAN|LAN
|server number|low (10~20)|High (100)|High (100)
|server gateway|load balancer|own router|Own router


    注：以上三种方法所能支持最大服务器数目的估计是假设调度器使用100M网卡，调度器的硬件配置与后端服务器的硬件配置相同，而且是对一般Web服务。使用更高的硬件配置（如千兆网卡和更快的处理器）作为调度器，调度器所能调度的服务器数量会相应增加。当应用不同时，服务器的数目也会相应地改变。所以，以上数据估计主要是为三种方法的伸缩性进行量化比较。

        * Virtual Server via NAT
        VS/NAT 的优点是服务器可以运行任何支持TCP/IP的操作系统，它只需要一个IP地址配置在调度器上，服务器组可以用私有的IP地址。缺点是它的伸缩能力有限，当服务器结点数目升到20时，调度器本身有可能成为系统的新瓶颈，因为在VS/NAT中请求和响应报文都需要通过负载调度器。 我们在Pentium 166 处理器的主机上测得重写报文的平均延时为60us，性能更高的处理器上延时会短一些。假设TCP报文的平均长度为536 Bytes，则调度器的最大吞吐量为8.93 MBytes/s. 我们再假设每台服务器的吞吐量为800KBytes/s，这样一个调度器可以带动10台服务器。（注：这是很早以前测得的数据）

        基于 VS/NAT的的集群系统可以适合许多服务器的性能要求。如果负载调度器成为系统新的瓶颈，可以有三种方法解决这个问题：混合方法、VS/TUN和 VS/DR。在DNS混合集群系统中，有若干个VS/NAT负载调度器，每个负载调度器带自己的服务器集群，同时这些负载调度器又通过RR-DNS组成简单的域名。但VS/TUN和VS/DR是提高系统吞吐量的更好方法。

        对于那些将IP地址或者端口号在报文数据中传送的网络服务，需要编写相应的应用模块来转换报文数据中的IP地址或者端口号。这会带来实现的工作量，同时应用模块检查报文的开销会降低系统的吞吐率。
        * Virtual Server via IP Tunneling
        在VS/TUN 的集群系统中，负载调度器只将请求调度到不同的后端服务器，后端服务器将应答的数据直接返回给用户。这样，负载调度器就可以处理大量的请求，它甚至可以调度百台以上的服务器（同等规模的服务器），而它不会成为系统的瓶颈。即使负载调度器只有100Mbps的全双工网卡，整个系统的最大吞吐量可超过 1Gbps。所以，VS/TUN可以极大地增加负载调度器调度的服务器数量。VS/TUN调度器可以调度上百台服务器，而它本身不会成为系统的瓶颈，可以用来构建高性能的超级服务器。

        VS/TUN技术对服务器有要求，即所有的服务器必须支持“IP Tunneling”或者“IP Encapsulation”协议。目前，VS/TUN的后端服务器主要运行Linux操作系统，我们没对其他操作系统进行测试。因为“IP Tunneling”正成为各个操作系统的标准协议，所以VS/TUN应该会适用运行其他操作系统的后端服务器。
    
        * Virtual Server via Direct Routing
        跟VS/TUN方法一样，VS/DR调度器只处理客户到服务器端的连接，响应数据可以直接从独立的网络路由返回给客户。这可以极大地提高LVS集群系统的伸缩性。

        跟VS/TUN相比，这种方法没有IP隧道的开销，但是要求负载调度器与实际服务器都有一块网卡连在同一物理网段上，服务器网络设备（或者设备别名）不作ARP响应，或者能将报文重定向（Redirect）到本地的Socket端口上。

### <a name="2.5">2.5 小结</a> 

    本章主要讲述了可伸缩网络服务LVS框架中的三种IP负载均衡技术。在分析网络地址转换方法（VS/NAT）的缺点和网络服务的非对称性的基础上，我们给出了通过IP隧道实现虚拟服务器的方法VS/TUN，和通过直接路由实现虚拟服务器的方法VS/DR，极大地提高了系统的伸缩性。



## <a name="3">3. 负载调度</a> 

    在上一章中，我们主要讲述了LVS集群中实现的三种IP负载均衡技术，它们主要解决系统的可伸缩性和透明性问题，如何通过负载调度器将请求高效地分发到不同的服务器执行，使得由多台独立计算机组成的集群系统成为一台虚拟服务器；客户端应用程序与集群系统交互时，就像与一台高性能的服务器交互一样。

    本章节将主要讲述在负载调度器上的负载调度策略和算法，如何将请求流调度到各台服务器，使得各台服务器尽可能地保持负载均衡。文章主要由两个部分组成。第一部分描述IP负载均衡软件IPVS在内核中所实现的各种连接调度算法；第二部分给出一个动态反馈负载均衡算法（Dynamic-feedback load balancing），它结合内核中的加权连接调度算法，根据动态反馈回来的负载信息来调整服务器的权值，来进一步避免服务器间的负载不平衡。

    在下面描述中，我们称客户的socket和服务器的socket之间的数据通讯为连接，无论它们是使用TCP还是UDP协议。对于UDP数据报文的调度，IPVS调度器也会为之建立调度记录并设置超时值（如5分钟）；在设定的时间内，来自同一地址（IP地址和端口）的UDP数据包会被调度到同一台服务器。


### <a name="3.1">3.1 内核中的连接调度算法</a> 

    IPVS在内核中的负载均衡调度是以连接为粒度的。在HTTP协议（非持久）中，每个对象从WEB服务器上获取都需要建立一个TCP连接，同一用户的不同请求会被调度到不同的服务器上，所以这种细粒度的调度在一定程度上可以避免单个用户访问的突发性引起服务器间的负载不平衡。

    在内核中的连接调度算法上，IPVS已实现了以下十种调度算法：
        *轮叫调度（Round-Robin Scheduling）
        *加权轮叫调度（Weighted Round-Robin Scheduling）
        *最小连接调度（Least-Connection Scheduling）
        *加权最小连接调度（Weighted Least-Connection Scheduling）
        *基于局部性的最少链接（Locality-Based Least Connections Scheduling）
        *带复制的基于局部性最少链接（Locality-Based Least Connections with Replication Scheduling）
        *目标地址散列调度（Destination Hashing Scheduling）
        *源地址散列调度（Source Hashing Scheduling）
        *最短预期延时调度（Shortest Expected Delay Scheduling）
        *不排队调度（Never Queue Scheduling）

    下面，我们先介绍这八种连接调度算法的工作原理和算法流程，会在以后的文章中描述怎么用它们。

#### <a name="3.1.1">3.1.1 轮叫调度（Round-Robin Scheduling）</a> 

    轮叫调度（Round Robin Scheduling）算法就是以轮叫的方式依次将请求调度不同的服务器，即每次调度执行i = (i + 1) mod n，并选出第i台服务器。算法的优点是其简洁性，它无需记录当前所有连接的状态，所以它是一种无状态调度。

    在系统实现时，我们引入了一个额外条件，当服务器的权值为零时，表示该服务器不可用而不被调度。这样做的目的是将服务器切出服务（如屏蔽服务器故障和系统维护），同时与其他加权算法保持一致。所以，算法要作相应的改动，它的算法流程如下：

### 轮叫调度算法流程 

```C
假设有一组服务器S = {S0, S1, …, Sn-1}，一个指示变量i表示上一次选择的
服务器，W(Si)表示服务器Si的权值。变量i被初始化为n-1，其中n > 0。

j = i;
do {
	j = (j + 1) mod n;
	if (W(Sj) > 0) {
		i = j;
		return Si;
	}
} while (j != i);
return NULL;
```
    轮叫调度算法假设所有服务器处理性能均相同，不管服务器的当前连接数和响应速度。该算法相对简单，不适用于服务器组中处理性能不一的情况，而且当请求服务时间变化比较大时，轮叫调度算法容易导致服务器间的负载不平衡。

    虽然Round-Robin DNS方法也是以轮叫调度的方式将一个域名解析到多个IP地址，但轮叫DNS方法的调度粒度是基于每个域名服务器的，域名服务器对域名解析的缓存会妨碍轮叫解析域名生效，这会导致服务器间负载的严重不平衡。这里，IPVS轮叫调度算法的粒度是基于每个连接的，同一用户的不同连接都会被调度到不同的服务器上，所以这种细粒度的轮叫调度要比DNS的轮叫调度优越很多。

#### <a name="3.1.2">3.1.2 加权轮叫调度（Weighted Round-Robin Scheduling）</a> 
#### <a name="3.1.3">3.1.3 最小连接调度（Least-Connection Scheduling）</a> 
#### <a name="3.1.4">3.1.4 加权最小连接调度（Weighted Least-Connection Scheduling）</a> 
#### <a name="3.1.5">3.1.5 基于局部性的最少链接（Locality-Based Least Connections Scheduling）</a> 
#### <a name="3.1.6">3.1.6 带复制的基于局部性最少链接（Locality-Based Least Connections with Replication Scheduling）</a>
#### <a name="3.1.7">3.1.7 目标地址散列调度（Destination Hashing Scheduling）</a> 
#### <a name="3.1.8">3.1.8 源地址散列调度（Source Hashing Scheduling）</a> 


### <a name="3.2">3.2 动态反馈负载均衡算法</a> 
#### <a name="3.2.1">3.2.1 连接调度</a> 
#### <a name="3.2.2">3.2.2 动态反馈负载均衡机制</a> 
#### <a name="3.2.3">3.2.3 综合负载</a> 
#### <a name="3.2.4">3.2.4 权值计算</a> 
#### <a name="3.2.5">3.2.5 一个实现例子</a> 


### <a name="3.3">3.3 小结</a> 



## <a name="4">4.  IP虚拟服务器的实现和性能测试</a> 
## <a name="5">5. 内核中的基于内容请求分发</a> 
## <a name="6">6. TCPHA的设计与实现</a> 
